---
title: "Extremely Fast Convergence Rates for Extremum Seeking Control with Polyak-Ruppert Averaging"
collection: publications
permalink: /publication/QuarticQSA-arxiv
excerpt: 'Stochastic approximation is a foundation for many algorithms found in machine learning and optimization. It is in general slow to converge:  the mean square error vanishes as $O(n^{-1})$. A deterministic counterpart known as quasi-stochastic approximation is a viable alternative in many applications, including gradient-free optimization and reinforcement learning.   It was assumed in prior research that the optimal achievable convergence rate is $O(n^{-2})$. It is shown in this paper that through design it is possible to obtain far faster convergence, of order $O(n^{-4+\delta})$, with $\delta>0$ arbitrary.   
Two techniques are introduced for the first time to achieve this rate of convergence.   The theory is also specialized within the context of gradient-free optimization, and tested on standard benchmarks.  The main results are based on a combination of novel application of results from number theory and techniques adapted from stochastic approximation theory.'
date: 2024-3-25
venue: 'arXiv'
paperurl: 'https://arxiv.org/pdf/2206.00814.pdf'
---

The contents above will be part of a list of publications, if the user clicks the link for the publication than the contents of section will be rendered as a full page, allowing you to provide more information about the paper for the reader. When publications are displayed as a single page, the contents of the above "citation" field will automatically be included below this section in a smaller font.
